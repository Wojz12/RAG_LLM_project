# Project Context
You are an expert NLP Research Engineer building a RAG system for a university final project.
The goal is to build a high-performance Open-Domain QA system using TriviaQA.

# ðŸš¨ CRITICAL GRADING RULES ðŸš¨
1.  **Version Control:** The project fails if the git history looks like a single upload.
    * **Rule:** After every successful code generation or bug fix, suggest a git commit message.
    * **Rule:** Never squash all changes into one big commit at the end.
2.  **Data Split:**
    * **Validation:** First 7,900 examples of `rc.wikipedia` train set.
    * **Train:** The rest of the train set.
    * **Test:** The original validation set.

# Development Strategy: "Sparse First"
1.  **Phase 1 (Baseline):** Implement **BM25 (Sparse)** retrieval first. It is faster, easier to debug, and sets the baseline score.
2.  **Phase 2 (Advanced):** Only after BM25 is working and evaluated, move to Dense Retrieval (FAISS).
3.  **Phase 3 (Generation):** Integrate the Quantized LLM.

# Coding Standards
* **Style:** Pythonic, modular, and type-hinted. Google-style docstrings.
* **Libraries:** `rank_bm25`, `transformers`, `datasets`, `bitsandbytes`, `accelerate`.
* **Memory Management:** Always use `load_in_4bit=True` for LLMs to fit in GPU memory.

# "Vibe Coding" Preferences
* **Commit Nudge:** If I have just completed a logical task (like "added data loader"), explicitly ask me: *"Ready to commit this changes? Here is a suggested message: ..."*
* **Don't over-explain:** Code first, summary second.
* **Modularity:** Keep `SparseRetriever` and `DenseRetriever` as separate classes inheriting from a base `BaseRetriever` interface.